{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e055e38c",
   "metadata": {},
   "source": [
    "# Markov Decision Processes (MDP)\n",
    "\n",
    "In this notebook, we will cover the fundamental concepts of Markov Decision Processes (MDPs) and Reinforcement Learning, including Agent, States, Actions, Rewards, Policy, types of RL, model types, transition probabilities, discount factor, and the MDP framework.\n",
    "\n",
    "## Agent, States, Actions, Rewards, Policy\n",
    "\n",
    "### Agent\n",
    "An agent is an entity that makes decisions by interacting with an environment to achieve a goal. It observes the state of the environment, takes actions, and receives rewards as feedback.\n",
    "\n",
    "### States\n",
    "A state represents the current situation or configuration of the environment. It is a snapshot of all relevant information needed by the agent to make a decision.\n",
    "\n",
    "### Actions\n",
    "Actions are the choices available to the agent at each state. The agent selects actions based on its policy to influence the environment.\n",
    "\n",
    "### Rewards\n",
    "Rewards are feedback signals received from the environment in response to the agent's actions. The goal of the agent is to maximize the cumulative reward over time.\n",
    "\n",
    "### Policy\n",
    "A policy defines the strategy used by the agent to decide which actions to take in each state. It can be deterministic (specific action for each state) or stochastic (probabilities assigned to actions).\n",
    "\n",
    "## Types of Reinforcement Learning\n",
    "\n",
    "### Episodic for Model-Based\n",
    "In episodic reinforcement learning, tasks are divided into episodes, each with a clear starting and ending point. The agent's experience is reset after each episode. Model-based methods use a model of the environment to make decisions.\n",
    "\n",
    "### Continuous for Model-Free\n",
    "In continuous reinforcement learning, tasks do not have a clear endpoint, and the agent continuously interacts with the environment. Model-free methods do not use a model of the environment; instead, they learn directly from interactions.\n",
    "\n",
    "## Types of Models\n",
    "\n",
    "### Model-Free\n",
    "Model-free methods learn the value of actions and states directly from experience without modeling the environment's dynamics. Examples include Q-learning and SARSA.\n",
    "\n",
    "### Model-Based\n",
    "Model-based methods build a model of the environment's dynamics and use this model to plan actions. They can simulate the environment to predict future states and rewards.\n",
    "\n",
    "## Transition Probabilities\n",
    "\n",
    "Transition probabilities represent the likelihood of transitioning from one state to another given a specific action. They are denoted as $P(s'|s, a)$, where $s$ is the current state, $a$ is the action taken, and $s'$ is the next state.\n",
    "\n",
    "## Discount Factor\n",
    "\n",
    "The discount factor (gamma, $\\gamma$) is used to weigh future rewards compared to immediate rewards. It ranges from 0 to 1. A discount factor close to 0 prioritizes immediate rewards, while a factor close to 1 values future rewards more.\n",
    "\n",
    "## MDP Framework\n",
    "\n",
    "The MDP framework provides a mathematical model for decision-making problems where outcomes are partly random and partly under the control of the agent. An MDP is defined by:\n",
    "\n",
    "- A set of states ($S$)\n",
    "- A set of actions ($A$)\n",
    "- A transition model ($P$): $P(s'|s, a)$\n",
    "- A reward function ($R$): $R(s, a, s')$\n",
    "- A discount factor ($\\gamma$)\n",
    "\n",
    "\n",
    "## Markov Decision Processes\n",
    "\n",
    "### Markov Decision Processes Formally Describe an Environment for Reinforcement Learning\n",
    "\n",
    "- The environment is fully observable.\n",
    "- The current state completely characterizes the process.\n",
    "- Almost all RL problems can be formalized as an MDP.\n",
    "- Partially observable problems can also be converted to MDPs.\n",
    "\n",
    "### Markov Property\n",
    "\n",
    "The future is independent of the past given the present.\n",
    "- All necessary information from the past is encapsulated in the present state, so there is no need to reconsider the past.\n",
    "\n",
    "A state $S_t$ is Markov if and only if:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    ">$$P(S_{t+1} | S_t) = P(S_{t+1} | S_1, \\ldots, S_t)$$\n",
    "\n",
    "\n",
    "---\n",
    "The state captures all relevant information from the history. Once the state is known, the history is irrelevant. \"The future is independent of the past given the present.\"\n",
    "\n",
    "### State Transition Matrix\n",
    "\n",
    "For a Markov state $s$ and a successor state $s'$, the state transition probability is defined as:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    ">$$P_{ss'} = P(S_{t+1} = s' | S_t = s)$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Markov Process\n",
    "\n",
    "A stochastic process involves some randomness in obtaining the outcome. We consider the states $S$ of our environment.\n",
    "- A Markov process is a sequence of states in this environment where the Markov property holds.\n",
    "A Markov process based on this environment consists of a tuple $(S, P) $ where:\n",
    "- $S$ is a finite set of states.\n",
    "- $P$ is the state transition probability matrix.\n",
    "\n",
    "### Markov Process with Rewards\n",
    "\n",
    "A Markov process with rewards is a Markov chain with values. We introduce values into the Markov process with states, forming the tuple $(S, P, R, \\gamma)$:\n",
    "- $S$ is a finite set of states.\n",
    "- $P$ is the state transition probability matrix.\n",
    "- $R$ is a reward function, $R_s = \\mathbb{E}[R_{t+1} | S_t = s]$.\n",
    "- $\\gamma$ is a discount factor, $\\gamma \\in [0, 1]$.\n",
    "\n",
    "### Return\n",
    "\n",
    "The return $G_t$ is the total discounted reward from time $t$:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    ">$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "- $\\gamma \\in [0, 1]$\n",
    "- $\\gamma$ close to 0 prioritizes immediate rewards.\n",
    "- $\\gamma$ close to 1 prioritizes future rewards.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Value Function\n",
    "\n",
    "The value function $V(s)$ gives the long-term value of the state $s$ or an action.\n",
    "The state value function $V(s)$ of an MRP is the expected return from state $s$:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    ">$$V(s) = \\mathbb{E}[G_t | S_t = s]$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Bellman Equation\n",
    "\n",
    "The value function can be decomposed into:\n",
    "- Immediate reward: $R_{t+1}$\n",
    "- Discounted value of the next state: $\\gamma V(S_{t+1})$\n",
    "\n",
    "Rewriting the equation introduces the state transition matrix $P_{ss'}$:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    ">$$V(s) = R_s + \\gamma \\sum_{s' \\in S} P_{ss'} V(s')$$\n",
    "\n",
    "### Matrix Form\n",
    "\n",
    "The Bellman equation can also be written in matrix form:\n",
    ">$$V = R + \\gamma P V$$\n",
    "\n",
    "The Bellman equation is a linear equation with the solution:\n",
    ">$$V = (1 - \\gamma P)^{-1} R$$\n",
    "\n",
    "### Solution Techniques\n",
    "\n",
    "- Dynamic Programming\n",
    "- Monte Carlo Methods\n",
    "- Temporal Difference Methods\n",
    "\n",
    "### Markov Decision Process\n",
    "\n",
    "A Markov Decision Process is a Markov reward process with decisions.\n",
    "- All states are also Markov.\n",
    "\n",
    "Introducing decisions into the Markov reward process forms the tuple $\\langle S, A, P, R, \\gamma \\rangle$:\n",
    "- $S$ is a finite set of states.\n",
    "- $A$ is a finite set of actions.\n",
    "- $P$ is the state transition probability matrix.\n",
    "- $R$ is a reward function, $R_s = \\mathbb{E}[R_{t+1} | S_t = s, A_t = a]$.\n",
    "- $\\gamma$ is a discount factor, $\\gamma \\in [0, 1]$.\n",
    "\n",
    "Now, let's explore these concepts with some Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7fa191",
   "metadata": {},
   "source": [
    "## Deterministic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7ae420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the states, actions, and rewards for a simple MDP\n",
    "states = ['S0', 'S1', 'S2']\n",
    "actions = ['A0', 'A1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc8bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition probabilities P(s'|s, a)\n",
    "transition_probabilities = {\n",
    "    'S0': {'A0': {'S0': 0.1, 'S1': 0.9},\n",
    "           'A1': {'S0': 0.7, 'S1': 0.3}},\n",
    "    'S1': {'A0': {'S1': 0.8, 'S2': 0.2},\n",
    "           'A1': {'S1': 0.4, 'S2': 0.6}},\n",
    "    'S2': {'A0': {'S0': 0.3, 'S2': 0.7},\n",
    "           'A1': {'S0': 0.4, 'S2': 0.6}}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a8ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards R(s, a, s')\n",
    "rewards = {\n",
    "    'S0': {'A0': {'S0': 0, 'S1': 10},\n",
    "           'A1': {'S0': 0, 'S1': 0}},\n",
    "    'S1': {'A0': {'S1': 0, 'S2': 10},\n",
    "           'A1': {'S1': 0, 'S2': 10}},\n",
    "    'S2': {'A0': {'S0': 0, 'S2': 10},\n",
    "           'A1': {'S0': 0, 'S2': 10}}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f58f13c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a policy (deterministic for simplicity)\n",
    "policy = {\n",
    "    'S0': 'A0',\n",
    "    'S1': 'A0',\n",
    "    'S2': 'A1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f99bb47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate one step in the MDP\n",
    "def step(state, action):\n",
    "    next_state = np.random.choice(list(transition_probabilities[state][action].keys()),\n",
    "                                  p=list(transition_probabilities[state][action].values()))\n",
    "    reward = rewards[state][action][next_state]\n",
    "    return next_state, reward\n",
    "\n",
    "# Simulate an episode\n",
    "def simulate_episode(start_state, policy, max_steps=10):\n",
    "    state = start_state\n",
    "    total_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        action = policy[state]\n",
    "        next_state, reward = step(state, action)\n",
    "        print(f\"State: {state}, Action: {action}, Reward: {reward}, Next State: {next_state}\")\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if state == 'S2':  # End of episode\n",
    "            break\n",
    "    print(f\"Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d19d2044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: S0, Action: A0, Reward: 10, Next State: S1\n",
      "State: S1, Action: A0, Reward: 10, Next State: S2\n",
      "Total Reward: 20\n"
     ]
    }
   ],
   "source": [
    "# Run a simulation\n",
    "simulate_episode('S0', policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c11f32",
   "metadata": {},
   "source": [
    "## Stochastic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e187910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: S0, Action: A1, Reward: 0, Next State: S1\n",
      "State: S1, Action: A1, Reward: 10, Next State: S2\n",
      "Total Reward: 10\n"
     ]
    }
   ],
   "source": [
    "# Define a policy (stochastic for this example)\n",
    "policy = {\n",
    "    'S0': {'A0': 0.5, 'A1': 0.5},\n",
    "    'S1': {'A0': 0.7, 'A1': 0.3},\n",
    "    'S2': {'A0': 0.4, 'A1': 0.6}\n",
    "}\n",
    "\n",
    "# Function to simulate one step in the MDP\n",
    "def step(state, action):\n",
    "    next_state = np.random.choice(list(transition_probabilities[state][action].keys()),\n",
    "                                  p=list(transition_probabilities[state][action].values()))\n",
    "    reward = rewards[state][action][next_state]\n",
    "    return next_state, reward\n",
    "\n",
    "# Simulate an episode with a stochastic policy\n",
    "def simulate_episode(start_state, policy, max_steps=10):\n",
    "    state = start_state\n",
    "    total_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        action = np.random.choice(list(policy[state].keys()), p=list(policy[state].values()))\n",
    "        next_state, reward = step(state, action)\n",
    "        print(f\"State: {state}, Action: {action}, Reward: {reward}, Next State: {next_state}\")\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        if state == 'S2':  # End of episode\n",
    "            break\n",
    "    print(f\"Total Reward: {total_reward}\")\n",
    "\n",
    "# Run a simulation with a stochastic policy\n",
    "simulate_episode('S0', policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
