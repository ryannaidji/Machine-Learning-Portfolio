{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c3db50",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods in Reinforcement Learning\n",
    "\n",
    "Monte Carlo methods are a class of algorithms that rely on repeated random sampling to obtain numerical results. In the context of reinforcement learning, Monte Carlo methods are used to estimate the value of states or state-action pairs based on observed episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f345213",
   "metadata": {},
   "source": [
    "## Monte Carlo Prediction\n",
    "\n",
    "Monte Carlo prediction methods are used to estimate the value function \\( V(s) \\) for a given policy $\\pi$ based on the average return from multiple episodes. The value of a state is updated by averaging the returns observed after visiting that state.\n",
    "\n",
    "### Process:\n",
    "1. **Generate Episodes**: Run the policy $\\pi$ to generate multiple episodes.\n",
    "2. **Calculate Returns**: For each state in each episode, calculate the return (cumulative reward) from that state to the end of the episode.\n",
    "3. **Update Value Function**: Average the returns for each state over multiple episodes to estimate the value function.\n",
    "\n",
    "### Methods:\n",
    "- **Every-Visit**: Updates the value function using the average of all returns observed each time a state is visited.\n",
    "- **First-Visit**: Updates the value function using the average of returns observed only the first time a state is visited in each episode.\n",
    "\n",
    "### Equation:\n",
    "---\n",
    "\n",
    "- **Every-Visit**:\n",
    ">$$V(s) \\approx \\frac{1}{N(s)} \\sum_{i=1}^{N(s)} G_i(s)$$\n",
    "\n",
    "- **First-Visit**:\n",
    ">$$V(s) \\approx \\frac{1}{N_{\\text{first}}(s)} \\sum_{i=1}^{N_{\\text{first}}(s)} G_i(s)$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $N(s)$ is the number of times state $s$ has been visited\n",
    "- $N_{\\text{first}}(s)$ is the number of times state $s$ has been visited for the first time in each episode, and $G_i(s)$ is the return observed from state $s$ in the $i$-th episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60695251",
   "metadata": {},
   "source": [
    "## Monte Carlo Control\n",
    "\n",
    "Monte Carlo control methods are used to find the optimal policy $pi^*$ by learning from episodes generated by the current policy. There are two main approaches: on-policy and off-policy learning.\n",
    "\n",
    "### On-policy Monte Carlo Control\n",
    "\n",
    "On-policy methods update the policy based on the actions taken by the current policy. The policy is improved iteratively using the estimated action-value function $Q(s, a)$.\n",
    "\n",
    "### Off-policy Monte Carlo Control\n",
    "\n",
    "Off-policy methods learn the value of the optimal policy $\\pi^*$ while following a different behavior policy $\\mu$. This approach uses importance sampling to correct for the difference between the behavior policy and the target policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b700ba",
   "metadata": {},
   "source": [
    "## Off-policy and On-policy Learning\n",
    "\n",
    "### On-policy Learning\n",
    "In on-policy learning, the policy used to generate the episodes is the same as the policy being improved. The action-value function $Q(s, a)$ is updated using the returns observed from the episodes generated by the current policy.\n",
    "\n",
    "### Off-policy Learning\n",
    "In off-policy learning, the behavior policy $\\mu$ is different from the target policy $\\pi$. The action-value function $Q(s, a)$ is updated using importance sampling to correct for the difference between the behavior policy and the target policy.\n",
    "\n",
    "### Importance Sampling\n",
    "Importance sampling is used to weigh the returns observed from the behavior policy \\( \\mu \\) to estimate the returns for the target policy \\( \\pi \\).\n",
    "\n",
    "### Equation:\n",
    "---\n",
    "\n",
    "\n",
    ">$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot \\frac{\\pi(a|s)}{\\mu(a|s)} \\left( G - Q(s, a) \\right)$$\n",
    "\n",
    "Where:\n",
    "- $G$ is the return observed from the episode \n",
    "- $\\alpha$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc18c6f",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Policy\n",
    "\n",
    "The epsilon-greedy policy is a method to balance exploration and exploitation. It ensures that the agent explores the environment by choosing random actions with probability $\\epsilon$ and exploits the best-known actions with probability $1 - \\epsilon$.\n",
    "\n",
    "### Epsilon-Greedy Policy Algorithm\n",
    "\n",
    "1. **Initialization**: Set $\\epsilon$ (exploration rate).\n",
    "2. **Action Selection**:\n",
    "   - With probability $\\epsilon$, choose a random action.\n",
    "   - With probability $1 - \\epsilon$, choose the action that maximizes the estimated value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c5f7e",
   "metadata": {},
   "source": [
    "## Monte Carlo Algorithm\n",
    "\n",
    "The Monte Carlo algorithm involves generating episodes, calculating returns, and updating value estimates based on empirical averages.\n",
    "\n",
    "### Pseudocode:\n",
    "\n",
    "1. Initialize value function $V(s)$ arbitrarily\n",
    "2. For each episode:\n",
    "    - Generate episode using policy $\\pi$\n",
    "    - Calculate return $G$ from state $s$\n",
    "    - Update $V(s)$ using empirical average of returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7439c4a2",
   "metadata": {},
   "source": [
    "### Monte Carlo Control Algorithm (On-policy):\n",
    "\n",
    "1. Initialize $Q(s, a)$ arbitrarily and $\\pi$ to be epsilon-greedy\n",
    "2. For each episode:\n",
    "    - Generate episode using policy $\\pi$\n",
    "    - For each state-action pair $(s, a)$ in episode:\n",
    "        - Calculate return $G$ from $(s, a)$\n",
    "        - Update $Q(s, a)$ using empirical average of returns\n",
    "        - Update policy $\\pi$ to be greedy with respect to $Q$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c62fa7",
   "metadata": {},
   "source": [
    "## Example Implementation of Monte Carlo Methods in Python\n",
    "\n",
    "Below is an example implementation of Monte Carlo prediction and control methods in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca0cd95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated State-Value Function:\n",
      "V(3) = -0.03\n",
      "V(1) = 0.11\n",
      "V(2) = 0.11\n",
      "V(0) = 0.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the environment and policy\n",
    "states = [0, 1, 2, 3, 4]\n",
    "actions = ['a', 'b']\n",
    "policy = {s: np.random.choice(actions) for s in states}\n",
    "\n",
    "# Simulate an environment\n",
    "def generate_episode(policy):\n",
    "    episode = []\n",
    "    state = np.random.choice(states)\n",
    "    while state != 4:  # Terminal state\n",
    "        action = policy[state]\n",
    "        next_state = np.random.choice(states)\n",
    "        reward = np.random.randn()  # Random reward\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "# Monte Carlo prediction (First-Visit)\n",
    "def monte_carlo_prediction_first_visit(policy, episodes, gamma=0.9):\n",
    "    V = defaultdict(float)\n",
    "    returns = defaultdict(list)\n",
    "    for _ in range(episodes):\n",
    "        episode = generate_episode(policy)\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, _, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            if state not in visited:\n",
    "                visited.add(state)\n",
    "                returns[state].append(G)\n",
    "                V[state] = np.mean(returns[state])\n",
    "    return V\n",
    "\n",
    "# Run Monte Carlo prediction (First-Visit)\n",
    "value_function = monte_carlo_prediction_first_visit(policy, episodes=1000)\n",
    "\n",
    "print(\"Estimated State-Value Function:\")\n",
    "for state, value in value_function.items():\n",
    "    print(f\"V({state}) = {value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106a23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy:\n",
      "State 0: a\n",
      "State 1: b\n",
      "State 2: b\n",
      "State 3: a\n",
      "State 4: b\n",
      "\n",
      "Estimated Action-Value Function:\n",
      "Q(2, a) = -0.50\n",
      "Q(2, b) = -0.06\n",
      "Q(0, a) = 0.12\n",
      "Q(0, b) = -1.13\n",
      "Q(1, a) = -2.84\n",
      "Q(1, b) = 0.33\n",
      "Q(3, a) = -0.33\n",
      "Q(3, b) = -0.80\n"
     ]
    }
   ],
   "source": [
    "# Monte Carlo control (on-policy, Every-Visit)\n",
    "def monte_carlo_control_on_policy(episodes, gamma=0.9, epsilon=0.1):\n",
    "    Q = defaultdict(lambda: np.zeros(len(actions)))\n",
    "    policy = {s: np.random.choice(actions) for s in states}\n",
    "\n",
    "    def epsilon_greedy_policy(state):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(actions)\n",
    "        else:\n",
    "            return actions[np.argmax(Q[state])]\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        episode = []\n",
    "        state = np.random.choice(states)\n",
    "        while state != 4:  # Terminal state\n",
    "            action = epsilon_greedy_policy(state)\n",
    "            next_state = np.random.choice(states)\n",
    "            reward = np.random.randn()  # Random reward\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:\n",
    "                Q[state][actions.index(action)] += (G - Q[state][actions.index(action)]) / len(episode)\n",
    "                policy[state] = actions[np.argmax(Q[state])]\n",
    "                \n",
    "    return policy, Q\n",
    "\n",
    "# Run Monte Carlo control (on-policy, Every-Visit)\n",
    "optimal_policy, action_value_function = monte_carlo_control_on_policy(episodes=1000)\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for state, action in optimal_policy.items():\n",
    "    print(f\"State {state}: {action}\")\n",
    "\n",
    "print(\"\\nEstimated Action-Value Function:\")\n",
    "for state, values in action_value_function.items():\n",
    "    for action, value in zip(actions, values):\n",
    "        print(f\"Q({state}, {action}) = {value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b96752",
   "metadata": {},
   "source": [
    "The estimated state-value function provides the expected return for each state under the given policy. The optimal policy shows the best action to take in each state based on the learned action-value function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
