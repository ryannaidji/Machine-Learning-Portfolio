{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8605f6",
   "metadata": {},
   "source": [
    "# Temporal Difference (TD) Learning in Reinforcement Learning\n",
    "\n",
    "Temporal Difference (TD) learning is a model-free approach in reinforcement learning that combines ideas from Monte Carlo methods and dynamic programming. It allows agents to learn directly from raw experience without needing a model of the environment's dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941161a5",
   "metadata": {},
   "source": [
    "## TD Prediction\n",
    "\n",
    "TD prediction methods are used to estimate the value function $V(s)$ for a given policy $\\pi$. The key feature of TD methods is that they update the value of a state based on the estimated value of the next state, rather than waiting for the final outcome.\n",
    "\n",
    "### TD(0) Update Rule:\n",
    "---\n",
    "\n",
    "\n",
    ">$$V(s) \\leftarrow V(s) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(s)]$$\n",
    "\n",
    "Where:\n",
    "- $V(s)$ is the current value of state $s$\n",
    "- $\\alpha$ is the learning rate\n",
    "- $R_{t+1}$ is the reward received after transitioning from state $s$ to state $S_{t+1}$\n",
    "- $\\gamma$ is the discount factor\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages of TD:\n",
    "- TD methods are model-free, meaning they do not require knowledge of the transition and reward functions of the MDP.\n",
    "- TD methods can learn from incomplete episodes, unlike Monte Carlo methods, which require complete episodes.\n",
    "- TD methods develop an estimate based on an estimate, allowing them to learn before knowing the final outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d107068",
   "metadata": {},
   "source": [
    "## TD Control\n",
    "\n",
    "TD control methods extend TD prediction to learn optimal policies. They use value functions to improve the policy iteratively.\n",
    "\n",
    "### Update Policy and Behavior Policy:\n",
    "- **Update Policy**: The policy used to update the value function.\n",
    "- **Behavior Policy**: The policy used to generate the behavior (actions) of the agent.\n",
    "\n",
    "### On-policy vs. Off-policy:\n",
    "- **On-policy**: The agent follows a single policy for both updating the value function and generating behavior.\n",
    "- **Off-policy**: The agent uses different policies for updating the value function and generating behavior.\n",
    "\n",
    "### On-policy Example: SARSA\n",
    "SARSA (State-Action-Reward-State-Action) is an on-policy TD control algorithm. It updates the Q-value based on the action actually taken by the current policy.\n",
    "\n",
    "### Off-policy Example: Q-Learning\n",
    "Q-Learning is an off-policy TD control algorithm. It updates the Q-value based on the maximum possible Q-value of the next state, independent of the action taken by the current policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043149e",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "\n",
    "SARSA is an on-policy TD control algorithm that updates the action-value function based on the current policy.\n",
    "\n",
    "### SARSA Update Rule:\n",
    "---\n",
    "\n",
    "\n",
    ">$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(s, a)]$$\n",
    "\n",
    "Where:\n",
    "- $Q(s, a)$ is the current value of taking action $a$ in state $s$\n",
    "- $\\alpha$ is the learning rate\n",
    "- $R_{t+1}$ is the reward received after taking action $a$ in state $s$\n",
    "- $\\gamma$ is the discount factor\n",
    "- $S_{t+1}$ is the next state\n",
    "- $A_{t+1}$ is the next action taken by the current policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf9f12f",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Q-Learning is an off-policy TD control algorithm that updates the action-value function based on the maximum possible Q-value of the next state.\n",
    "\n",
    "### Q-Learning Update Rule:\n",
    "---\n",
    "\n",
    "\n",
    ">$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(s, a)]$$\n",
    "\n",
    "Where:\n",
    "- $Q(s, a)$ is the current value of taking action $a$ in state $s$\n",
    "- $\\alpha$ is the learning rate\n",
    "- $R_{t+1}$ is the reward received after taking action $a$ in state $s$\n",
    "- $\\gamma$ is the discount factor\n",
    "- $S_{t+1}$ is the next state\n",
    "- $\\max_{a'} Q(S_{t+1}, a')$ is the maximum Q-value of the next state over all possible actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fcda2b",
   "metadata": {},
   "source": [
    "## Example Implementation of SARSA and Q-Learning in Python\n",
    "\n",
    "Below is an example implementation of the SARSA and Q-Learning algorithms in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca5608b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Q-Values (SARSA):\n",
      "Q(4, a) = 0.00\n",
      "Q(4, b) = 0.00\n",
      "Q(3, a) = -0.05\n",
      "Q(3, b) = -0.35\n",
      "Q(0, a) = -0.26\n",
      "Q(0, b) = 0.17\n",
      "Q(1, a) = -0.08\n",
      "Q(1, b) = -0.04\n",
      "Q(2, a) = -0.15\n",
      "Q(2, b) = -0.27\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the environment and policy\n",
    "states = [0, 1, 2, 3, 4]\n",
    "actions = ['a', 'b']\n",
    "\n",
    "# Simulate an environment\n",
    "def generate_episode(policy):\n",
    "    episode = []\n",
    "    state = np.random.choice(states)\n",
    "    while state != 4:  # Terminal state\n",
    "        action = policy[state]\n",
    "        next_state = np.random.choice(states)\n",
    "        reward = np.random.randn()  # Random reward\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "# Epsilon-greedy policy\n",
    "def epsilon_greedy_policy(Q, state, epsilon=0.1):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(actions)\n",
    "    else:\n",
    "        return actions[np.argmax(Q[state])]\n",
    "\n",
    "# SARSA algorithm\n",
    "def sarsa(episodes, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    Q = defaultdict(lambda: np.zeros(len(actions)))\n",
    "    for _ in range(episodes):\n",
    "        state = np.random.choice(states)\n",
    "        action = epsilon_greedy_policy(Q, state, epsilon)\n",
    "        while state != 4:  # Terminal state\n",
    "            next_state = np.random.choice(states)\n",
    "            reward = np.random.randn()  # Random reward\n",
    "            next_action = epsilon_greedy_policy(Q, next_state, epsilon)\n",
    "            Q[state][actions.index(action)] += alpha * (reward + gamma * Q[next_state][actions.index(next_action)] - Q[state][actions.index(action)])\n",
    "            state, action = next_state, next_action\n",
    "    return Q\n",
    "\n",
    "# Run SARSA\n",
    "Q_sarsa = sarsa(episodes=1000)\n",
    "\n",
    "print(\"Estimated Q-Values (SARSA):\")\n",
    "for state, values in Q_sarsa.items():\n",
    "    for action, value in zip(actions, values):\n",
    "        print(f\"Q({state}, {action}) = {value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f1cfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Q-Values (Q-Learning):\n",
      "Q(0, a) = 0.05\n",
      "Q(0, b) = 0.19\n",
      "Q(4, a) = 0.00\n",
      "Q(4, b) = 0.00\n",
      "Q(2, a) = -0.12\n",
      "Q(2, b) = 0.12\n",
      "Q(3, a) = -0.03\n",
      "Q(3, b) = -0.14\n",
      "Q(1, a) = -0.19\n",
      "Q(1, b) = 0.23\n"
     ]
    }
   ],
   "source": [
    "# Q-Learning algorithm\n",
    "def q_learning(episodes, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    Q = defaultdict(lambda: np.zeros(len(actions)))\n",
    "    for _ in range(episodes):\n",
    "        state = np.random.choice(states)\n",
    "        while state != 4:  # Terminal state\n",
    "            action = epsilon_greedy_policy(Q, state, epsilon)\n",
    "            next_state = np.random.choice(states)\n",
    "            reward = np.random.randn()  # Random reward\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            Q[state][actions.index(action)] += alpha * (reward + gamma * Q[next_state][best_next_action] - Q[state][actions.index(action)])\n",
    "            state = next_state\n",
    "    return Q\n",
    "\n",
    "# Run Q-Learning\n",
    "Q_q_learning = q_learning(episodes=1000)\n",
    "\n",
    "print(\"Estimated Q-Values (Q-Learning):\")\n",
    "for state, values in Q_q_learning.items():\n",
    "    for action, value in zip(actions, values):\n",
    "        print(f\"Q({state}, {action}) = {value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e48105",
   "metadata": {},
   "source": [
    "## Explanation of the Temporal Difference Learning Implementation\n",
    "\n",
    "### SARSA\n",
    "- **Initialization**: We initialize the Q-values for all state-action pairs.\n",
    "- **Generate Episodes**: The agent follows an epsilon-greedy policy to generate episodes.\n",
    "- **Update Q-Values**: The Q-values are updated based on the actual action taken by the policy.\n",
    "\n",
    "### Q-Learning\n",
    "- **Initialization**: We initialize the Q-values for all state-action pairs.\n",
    "- **Generate Episodes**: The agent follows an epsilon-greedy policy to generate episodes.\n",
    "- **Update Q-Values**: The Q-values are updated based on the maximum Q-value of the next state, independent of the action taken by the policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640e0a4",
   "metadata": {},
   "source": [
    "The estimated Q-values provide the expected return for each state-action pair under the given policy. The SARSA algorithm updates the Q-values based on the action actually taken by the policy, while the Q-Learning algorithm updates the Q-values based on the maximum possible Q-value of the next state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
