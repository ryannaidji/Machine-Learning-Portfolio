{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee029a0c",
   "metadata": {},
   "source": [
    "# Policy in Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b16690f",
   "metadata": {},
   "source": [
    "## Definition of Policy\n",
    "\n",
    "In reinforcement learning, a policy defines the agent's way of behaving at a given time. It is a mapping from states of the environment to actions to be taken when in those states. The policy is often denoted by $\\pi$.\n",
    "\n",
    "Mathematically, a policy $\\pi$ is defined as:\n",
    "\n",
    "- **Deterministic Policy**: A deterministic policy maps each state to a specific action. Formally, $\\pi(s) = a$, where $s$ is a state and $a$ is an action.\n",
    "- **Stochastic Policy**: A stochastic policy gives a probability distribution over actions for each state. Formally, $\\pi(a|s) = P[A_t = a | S_t = s]$, which is the probability that action $a$ is taken in state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e64443b",
   "metadata": {},
   "source": [
    "## Deterministic and Stochastic Policies\n",
    "\n",
    "### Deterministic Policy\n",
    "\n",
    "In a deterministic policy, the action is determined with certainty given a state. This can be represented as:\n",
    "$\\pi(s) = a$\n",
    "\n",
    "### Stochastic Policy\n",
    "\n",
    "In a stochastic policy, the action is determined based on a probability distribution given a state. This can be represented as:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    ">$$\\pi(a|s) = P[A_t = a | S_t = s]$$\n",
    "\n",
    "Where:\n",
    "- $\\pi(a|s)$ is the probability of taking action $a$ in state $s$.\n",
    "- $A_t$ is the action at time step t.\n",
    "- $S_t$ is the state at time step t.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123d2ee2",
   "metadata": {},
   "source": [
    "## Example of a Policy\n",
    "\n",
    "Consider a 3x3 grid where an agent can move in four directions: right, down, left, and up. The following matrix represents the policy for the agent, where each element corresponds to an action to be taken in a specific state (grid cell):\n",
    "\n",
    "```\n",
    "[ [right, down, down],\n",
    "[right, right, down],\n",
    "[right, right, up] ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59170f61",
   "metadata": {},
   "source": [
    "### Grid Representation\n",
    "\n",
    "- Each cell in the matrix represents a state.\n",
    "- The action to be taken in each state is represented by the direction the agent should move.\n",
    "\n",
    "### Explanation of Actions\n",
    "\n",
    "- **Row 1**: \n",
    "  - From the top-left corner (state (0,0)), the agent should move `right` to (0,1).\n",
    "  - From state (0,1), the agent should move `down` to (1,1).\n",
    "  - From state (0,2), the agent should move `down` to (1,2).\n",
    "\n",
    "- **Row 2**:\n",
    "  - From state (1,0), the agent should move `right` to (1,1).\n",
    "  - From state (1,1), the agent should move `right` to (1,2).\n",
    "  - From state (1,2), the agent should move `down` to (2,2).\n",
    "\n",
    "- **Row 3**:\n",
    "  - From state (2,0), the agent should move `right` to (2,1).\n",
    "  - From state (2,1), the agent should move `right` to (2,2).\n",
    "  - From state (2,2), the agent should move `up` to (1,2).\n",
    "\n",
    "This policy directs the agent through the grid in a way that each state has a specific action associated with it, which is a characteristic of a deterministic policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce036aa",
   "metadata": {},
   "source": [
    "## State-Value Function\n",
    "\n",
    "The state-value function $V(s)$ gives the expected return (cumulative reward) starting from a state $s$ and following a specific policy $\\pi$. \n",
    "\n",
    "For our example grid, the state-value function matrix $V(s)$ can be represented as:\n",
    "\n",
    "```\n",
    "[ [4, 3, 3],\n",
    "[3, 2, 1],\n",
    "[3, 1, 0] ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f173349",
   "metadata": {},
   "source": [
    "\n",
    "### Explanation of State-Value Function Matrix\n",
    "\n",
    "- Each cell in the matrix represents the value of that state under the given policy.\n",
    "- The values indicate the expected return starting from that state and following the policy until reaching the goal.\n",
    "\n",
    "### Interpretation of Values\n",
    "\n",
    "- **Row 1**:\n",
    "  - $V(0,0) = 4$: Starting from state (0,0) and following the policy, the expected return is 4.\n",
    "  - $V(0,1) = 3$: Starting from state (0,1) and following the policy, the expected return is 3.\n",
    "  - $V(0,2) = 3$: Starting from state (0,2) and following the policy, the expected return is 3.\n",
    "\n",
    "- **Row 2**:\n",
    "  - $V(1,0) = 3$: Starting from state (1,0) and following the policy, the expected return is 3.\n",
    "  - $V(1,1) = 2$: Starting from state (1,1) and following the policy, the expected return is 2.\n",
    "  - $V(1,2) = 1$: Starting from state (1,2) and following the policy, the expected return is 1.\n",
    "\n",
    "- **Row 3**:\n",
    "  - $V(2,0) = 3$: Starting from state (2,0) and following the policy, the expected return is 3.\n",
    "  - $V(2,1) = 1$: Starting from state (2,1) and following the policy, the expected return is 1.\n",
    "  - $V(2,2) = 0$: Starting from state (2,2) and following the policy, the expected return is 0.\n",
    "\n",
    "This state-value function helps in evaluating how good the given policy is in terms of the expected returns from each state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581df4af",
   "metadata": {},
   "source": [
    "## Evaluation of Policy\n",
    "\n",
    "Evaluating a policy involves determining how good a policy is in achieving the goal of maximizing the cumulative reward. This is typically done using two main functions:\n",
    "\n",
    "- **State-Value Function (V)**: This function gives the expected return (cumulative reward) starting from a state $s$ and following policy $\\pi$. It is defined as:\n",
    "---\n",
    "\n",
    "\n",
    ">$$V^{\\pi}(s) = \\mathbb{E}_{\\pi} [R_t | S_t = s]$$\n",
    "\n",
    "\n",
    "---\n",
    "- **Action-Value Function (Q)**: This function gives the expected return starting from a state s, taking an action a, and then following policy Ï€. It is defined as:\n",
    "---\n",
    "\n",
    "\n",
    ">$$Q^{\\pi}(s, a) = \\mathbb{E}_{\\pi} [R_t | S_t = s, A_t = a]$$\n",
    "\n",
    "Where:\n",
    "- $R_t$ is the reward at time step t.\n",
    "- $S_t$ is the state at time step t.\n",
    "- $A_t$ is the action at time step t.\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
