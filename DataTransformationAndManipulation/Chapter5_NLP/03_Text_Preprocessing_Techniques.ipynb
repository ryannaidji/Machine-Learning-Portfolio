{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed80d450",
   "metadata": {},
   "source": [
    "\n",
    "# Text Preprocessing Techniques\n",
    "\n",
    "In this notebook, we will explore various text preprocessing techniques that are essential for preparing text data for Natural Language Processing (NLP) tasks. These techniques include tokenization, stemming, lemmatization, and named entity recognition (NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32265109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ryann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ryann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ryann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ryann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ryann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ne_chunk, pos_tag\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Sample text\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion. This move aims to expand their operations in Europe.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e44f203",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into individual tokens, which can be words, phrases, or other meaningful elements. This is the first step in text preprocessing and is crucial for further analysis.\n",
    "\n",
    "- **Word Tokenization**: Splitting a sentence into words.\n",
    "- **Sentence Tokenization**: Splitting a paragraph into sentences.\n",
    "\n",
    "Tokenization helps in breaking down the text into manageable pieces, making it easier to analyze and process further. It forms the basis for other preprocessing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b38adeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens:\n",
      " ['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.', 'This', 'move', 'aims', 'to', 'expand', 'their', 'operations', 'in', 'Europe', '.']\n",
      "\n",
      "Sentence Tokens:\n",
      " ['Apple is looking at buying U.K. startup for $1 billion.', 'This move aims to expand their operations in Europe.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "\n",
    "# Word tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\\n\", word_tokens)\n",
    "\n",
    "# Sentence tokenization\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(\"\\nSentence Tokens:\\n\", sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f7228",
   "metadata": {},
   "source": [
    "## 2. Stemming\n",
    "\n",
    "Stemming is the process of reducing words to their root form by removing suffixes. This helps in normalizing the text data by grouping similar words together.\n",
    "\n",
    "- **Example**: The words \"running\", \"runner\", and \"ran\" can be reduced to the root word \"run\".\n",
    "\n",
    "Stemming is a rule-based process that often results in non-dictionary forms of words. It is useful for applications where speed and simplicity are more important than precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c06c454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed Tokens:\n",
      " ['appl', 'is', 'look', 'at', 'buy', 'u.k.', 'startup', 'for', '$', '1', 'billion', '.', 'thi', 'move', 'aim', 'to', 'expand', 'their', 'oper', 'in', 'europ', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "\n",
    "# Initializing the PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Applying stemming to word tokens\n",
    "stemmed_tokens = [stemmer.stem(word) for word in word_tokens]\n",
    "print(\"\\nStemmed Tokens:\\n\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb7264",
   "metadata": {},
   "source": [
    "## 3. Lemmatization\n",
    "\n",
    "Lemmatization is the process of reducing words to their base or dictionary form. Unlike stemming, lemmatization takes into account the context and converts the word to its meaningful base form.\n",
    "\n",
    "- **Example**: The words \"running\" and \"ran\" are both converted to \"run\".\n",
    "\n",
    "Lemmatization uses vocabulary and morphological analysis of words, making it more accurate than stemming. It is particularly useful for applications where understanding the meaning of the words is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2beb1b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Tokens:\n",
      " ['Apple', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion', '.', 'This', 'move', 'aim', 'to', 'expand', 'their', 'operation', 'in', 'Europe', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "# Initializing the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Applying lemmatization to word tokens\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "print(\"\\nLemmatized Tokens:\\n\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d188a24",
   "metadata": {},
   "source": [
    "## 4. Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition (NER) is a technique used to identify and classify named entities (such as names of people, organizations, locations, dates, etc.) in the text. This helps in extracting important information from unstructured text.\n",
    "\n",
    "- **Example**: In the sentence \"Apple is looking at buying U.K. startup for $1 billion\", NER will identify \"Apple\" as an organization, \"U.K.\" as a location, and \"$1 billion\" as a monetary value.\n",
    "\n",
    "NER is useful for information retrieval, question answering, and summarization tasks, as it helps in identifying and categorizing key information from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83af35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part-of-Speech Tags:\n",
      " [('Apple', 'NNP'), ('is', 'VBZ'), ('looking', 'VBG'), ('at', 'IN'), ('buying', 'VBG'), ('U.K.', 'NNP'), ('startup', 'NN'), ('for', 'IN'), ('$', '$'), ('1', 'CD'), ('billion', 'CD'), ('.', '.'), ('This', 'DT'), ('move', 'NN'), ('aims', 'VBZ'), ('to', 'TO'), ('expand', 'VB'), ('their', 'PRP$'), ('operations', 'NNS'), ('in', 'IN'), ('Europe', 'NNP'), ('.', '.')]\n",
      "\n",
      "Named Entities:\n",
      " (S\n",
      "  (GPE Apple/NNP)\n",
      "  is/VBZ\n",
      "  looking/VBG\n",
      "  at/IN\n",
      "  buying/VBG\n",
      "  U.K./NNP\n",
      "  startup/NN\n",
      "  for/IN\n",
      "  $/$\n",
      "  1/CD\n",
      "  billion/CD\n",
      "  ./.\n",
      "  This/DT\n",
      "  move/NN\n",
      "  aims/VBZ\n",
      "  to/TO\n",
      "  expand/VB\n",
      "  their/PRP$\n",
      "  operations/NNS\n",
      "  in/IN\n",
      "  (GPE Europe/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "\n",
    "# POS tagging before NER\n",
    "pos_tags = pos_tag(word_tokens)\n",
    "print(\"\\nPart-of-Speech Tags:\\n\", pos_tags)\n",
    "\n",
    "# Applying NER\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "print(\"\\nNamed Entities:\\n\", named_entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
