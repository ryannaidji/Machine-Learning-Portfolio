{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4edfd40c",
   "metadata": {},
   "source": [
    "# Preparing Unstructured Text Data\n",
    "\n",
    "In this notebook, we will explore various techniques for preparing unstructured text data for analysis. These preprocessing steps are crucial for ensuring that the text data is clean and suitable for further NLP tasks.\n",
    "\n",
    "## 1. Corpus Cleaning\n",
    "\n",
    "Corpus cleaning involves several steps to standardize and clean the text data, making it more suitable for analysis. This includes:\n",
    "\n",
    "- **Removing Numbers**: Numbers can add noise to the text data, especially if they are not relevant to the analysis. Removing numbers helps in focusing on the textual content.\n",
    "\n",
    "- **Correcting Spelling**: Spelling errors can cause issues in text analysis by increasing the vocabulary size unnecessarily. Correcting these errors improves the quality of the text data.\n",
    "\n",
    "- **Harmonizing Case**: Text data can have inconsistent casing (uppercase, lowercase, mixed case), which can lead to treating the same word as different tokens. Converting the text to a consistent case (usually lowercase) helps in standardizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8905e615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ryann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ryann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ryann\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from autocorrect import Speller\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Text': [\n",
    "        'This is a sample text! It includes numbers like 123 and URLs like https://example.com.',\n",
    "        'Another example text with UPPERCASE letters and misspelled wrds.',\n",
    "        'Final example: punctuation, numbers (456), and mixedCASE text.'\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5817100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "                                                 Text\n",
      "0  This is a sample text! It includes numbers lik...\n",
      "1  Another example text with UPPERCASE letters an...\n",
      "2  Final example: punctuation, numbers (456), and...\n"
     ]
    }
   ],
   "source": [
    "# Display the original data\n",
    "print(\"Original Data:\\n\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4d1801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Corpus Cleaning:\n",
      "                                                 Text  \\\n",
      "0  This is a sample text! It includes numbers lik...   \n",
      "1  Another example text with UPPERCASE letters an...   \n",
      "2  Final example: punctuation, numbers (456), and...   \n",
      "\n",
      "                                        Cleaned_Text  \n",
      "0  this is a sample text! it includes numbers lik...  \n",
      "1  another example text with uppercase letters an...  \n",
      "2  final example: punctuation, numbers (), and mi...  \n"
     ]
    }
   ],
   "source": [
    "# Corpus Cleaning\n",
    "\n",
    "# Function to clean corpus\n",
    "def clean_text(text):\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Correct spelling\n",
    "    spell = Speller()\n",
    "    text = ' '.join([spell(word) for word in text.split()])\n",
    "    # Harmonize case\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df['Cleaned_Text'] = df['Text'].apply(clean_text)\n",
    "print(\"\\nAfter Corpus Cleaning:\\n\", df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f8045",
   "metadata": {},
   "source": [
    "## 2. Removing Punctuation, Stopwords, Numbers, and URLs\n",
    "\n",
    "Removing unnecessary elements such as punctuation, stopwords, numbers, and URLs helps in focusing on the relevant parts of the text data. Here's a breakdown:\n",
    "\n",
    "- **Punctuation**: Punctuation marks are often irrelevant for text analysis and can be removed to simplify the text.\n",
    "\n",
    "- **Stopwords**: Stopwords are common words (e.g., \"the\", \"is\", \"in\") that do not carry significant meaning and can be removed to focus on the more important words in the text. Stopwords vary by language, and different languages have their own stopword dictionaries. For example, NLTK provides stopword lists for several languages including English, French, Spanish, German, and more.\n",
    "\n",
    "- **Numbers**: As mentioned earlier, numbers can add noise to the text data. Removing them can help in concentrating on the textual content.\n",
    "\n",
    "- **URLs**: URLs in text data are usually not useful for analysis and can be removed to clean the text further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60de7c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Removing Punctuation, Stopwords, Numbers, and URLs:\n",
      "                                                 Text  \\\n",
      "0  This is a sample text! It includes numbers lik...   \n",
      "1  Another example text with UPPERCASE letters an...   \n",
      "2  Final example: punctuation, numbers (456), and...   \n",
      "\n",
      "                                        Cleaned_Text  \\\n",
      "0  this is a sample text! it includes numbers lik...   \n",
      "1  another example text with uppercase letters an...   \n",
      "2  final example: punctuation, numbers (), and mi...   \n",
      "\n",
      "                                   Preprocessed_Text  \n",
      "0         sample text includes numbers like us like   \n",
      "1  another example text uppercase letters misspel...  \n",
      "2   final example punctuation numbers mixedcase text  \n"
     ]
    }
   ],
   "source": [
    "# Removing Punctuation, Stopwords, Numbers, and URLs\n",
    "\n",
    "# Function to remove punctuation, stopwords, numbers, and URLs\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    text = ' '.join([word for word in word_tokens if word.lower() not in stop_words])\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "df['Preprocessed_Text'] = df['Cleaned_Text'].apply(preprocess_text)\n",
    "print(\"\\nAfter Removing Punctuation, Stopwords, Numbers, and URLs:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7df29",
   "metadata": {},
   "source": [
    "## 3. Converting to Lowercase\n",
    "\n",
    "Converting all text to lowercase ensures uniformity and helps in reducing the complexity of text data. This step is crucial because:\n",
    "\n",
    "- **Uniformity**: Treats words with different cases (e.g., \"Apple\" vs. \"apple\") as the same token, reducing redundancy.\n",
    "\n",
    "- **Simplification**: Simplifies the text data, making it easier to work with in subsequent analysis steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c5569f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Converting to Lowercase:\n",
      "                                                 Text  \\\n",
      "0  This is a sample text! It includes numbers lik...   \n",
      "1  Another example text with UPPERCASE letters an...   \n",
      "2  Final example: punctuation, numbers (456), and...   \n",
      "\n",
      "                                        Cleaned_Text  \\\n",
      "0  this is a sample text! it includes numbers lik...   \n",
      "1  another example text with uppercase letters an...   \n",
      "2  final example: punctuation, numbers (), and mi...   \n",
      "\n",
      "                                   Preprocessed_Text  \\\n",
      "0         sample text includes numbers like us like    \n",
      "1  another example text uppercase letters misspel...   \n",
      "2   final example punctuation numbers mixedcase text   \n",
      "\n",
      "                                     Lowercased_Text  \n",
      "0         sample text includes numbers like us like   \n",
      "1  another example text uppercase letters misspel...  \n",
      "2   final example punctuation numbers mixedcase text  \n"
     ]
    }
   ],
   "source": [
    "# Converting to Lowercase\n",
    "\n",
    "# Function to convert text to lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "df['Lowercased_Text'] = df['Preprocessed_Text'].apply(to_lowercase)\n",
    "print(\"\\nAfter Converting to Lowercase:\\n\", df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
