{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90be453a",
   "metadata": {},
   "source": [
    "# Text Vectorization\n",
    "\n",
    "In this notebook, we will explore various text vectorization techniques that are essential for converting text data into numerical representations for machine learning models. These techniques include TF-IDF, Bag of Words, and Word Embeddings (Word2Vec, GloVe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6125640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Sample text data\n",
    "documents = [\n",
    "    \"Apple is looking at buying U.K. startup for $1 billion\",\n",
    "    \"Apple bought a startup\",\n",
    "    \"Startup in the U.K. received $1 billion from Apple\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dbfbda",
   "metadata": {},
   "source": [
    "## 1. TF-IDF Method\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). The TF-IDF value increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "- **Term Frequency (TF)**: The number of times a word appears in a document. It measures how frequently a term occurs in a document. The more frequently a term appears in a document, the higher its TF value.\n",
    "---\n",
    "\n",
    "\n",
    ">$$TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}$$\n",
    "\n",
    "\n",
    "---\n",
    "- **Inverse Document Frequency (IDF)**: The logarithmically scaled inverse fraction of the documents that contain the word. It measures how important a term is. The more documents a term appears in, the lower its IDF value.\n",
    "---\n",
    "\n",
    "\n",
    ">$$IDF(t) = \\log \\left( \\frac{\\text{Total number of documents}}{\\text{Number of documents with term } t} \\right)$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The final TF-IDF score for a term in a document is calculated as:\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    ">$$TF\\text{-}IDF(t, d) = TF(t, d) \\times IDF(t)$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc733a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix (first 10 columns):\n",
      "       apple        at   billion    bought    buying       for      from  \\\n",
      "0  0.235756  0.399169  0.303578  0.000000  0.399169  0.399169  0.000000   \n",
      "1  0.453295  0.000000  0.000000  0.767495  0.000000  0.000000  0.000000   \n",
      "2  0.257129  0.000000  0.331100  0.000000  0.000000  0.000000  0.435357   \n",
      "\n",
      "         in        is   looking  \n",
      "0  0.000000  0.399169  0.399169  \n",
      "1  0.000000  0.000000  0.000000  \n",
      "2  0.435357  0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Method\n",
    "\n",
    "# Initializing TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transforming documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Converting to DataFrame for better readability\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix (first 10 columns):\\n\", tfidf_df.iloc[:, :10])  # Displaying only the first 10 columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c3c755",
   "metadata": {},
   "source": [
    "## 2. Other Vectorization Methods\n",
    "\n",
    "### Bag of Words\n",
    "\n",
    "The Bag of Words (BoW) model represents text data as a collection of words, disregarding grammar and word order but keeping multiplicity. It creates a vocabulary of all the unique words in the text and represents each document by counting the number of times each word appears.\n",
    "\n",
    "- **Process**:\n",
    "  1. Create a list of unique words (vocabulary) from the entire corpus.\n",
    "  2. Represent each document as a vector with the length of the vocabulary, where each position corresponds to the count of a word in the document.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Simple to implement and understand.\n",
    "  - Effective for basic text representation and simple models.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Ignores the context and order of words.\n",
    "  - Can lead to large and sparse matrices, especially for large vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e3cef09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag of Words Matrix (first 10 columns):\n",
      "    apple  at  billion  bought  buying  for  from  in  is  looking\n",
      "0      1   1        1       0       1    1     0   0   1        1\n",
      "1      1   0        0       1       0    0     0   0   0        0\n",
      "2      1   0        1       0       0    0     1   1   0        0\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words\n",
    "\n",
    "# Initializing Count Vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Transforming documents\n",
    "count_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Converting to DataFrame for better readability\n",
    "count_df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "print(\"\\nBag of Words Matrix (first 10 columns):\\n\", count_df.iloc[:, :10])  # Displaying only the first 10 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4425b7f0",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "Word embeddings are dense vector representations of words that capture the semantic meaning of words in a continuous vector space. They are trained on large corpora of text and can capture context, synonyms, and relationships between words.\n",
    "\n",
    "- **Word2Vec**: A neural network-based method that learns vector representations of words based on their context in the text. It uses two architectures: Continuous Bag of Words (CBOW) and Skip-Gram.\n",
    "  - **CBOW**: Predicts the target word (center word) from the context words (surrounding words).\n",
    "  - **Skip-Gram**: Predicts the context words (surrounding words) from the target word (center word).\n",
    "\n",
    "- **GloVe (Global Vectors for Word Representation)**: A method that leverages word co-occurrence statistics across the entire corpus to learn word vectors. It creates a co-occurrence matrix and factorizes it to obtain the word vectors.\n",
    "\n",
    "- **Advantages**:\n",
    "  - Captures semantic relationships and context.\n",
    "  - Produces dense and low-dimensional vectors, reducing computational complexity.\n",
    "\n",
    "- **Disadvantages**:\n",
    "  - Requires large corpora and significant computational resources to train.\n",
    "  - Pre-trained models may not capture domain-specific nuances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e0c6c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec Vector for 'Apple' (first 10 dimensions):\n",
      " [-0.00053623  0.00023643  0.00510335  0.00900927 -0.00930295 -0.00711681\n",
      "  0.00645887  0.00897299 -0.00501543 -0.00376337]\n",
      "\n",
      "GloVe Vector for 'Apple' (first 10 dimensions):\n",
      " [-0.5985   -0.46321   0.13001  -0.019576  0.4603   -0.3018    0.8977\n",
      " -0.65634   0.66858  -0.49164 ]\n"
     ]
    }
   ],
   "source": [
    "# Word Embeddings: Word2Vec\n",
    "\n",
    "# Using Gensim to train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=[doc.split() for doc in documents], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Displaying vector for a word (limited to first 10 dimensions for readability)\n",
    "print(\"\\nWord2Vec Vector for 'Apple' (first 10 dimensions):\\n\", word2vec_model.wv['Apple'][:10])\n",
    "\n",
    "#### Word Embeddings: GloVe\n",
    "\n",
    "# Loading pre-trained GloVe vectors\n",
    "glove_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# Displaying vector for a word (limited to first 10 dimensions for readability)\n",
    "print(\"\\nGloVe Vector for 'Apple' (first 10 dimensions):\\n\", glove_vectors['apple'][:10])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
